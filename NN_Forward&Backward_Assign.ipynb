{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8696805-784e-4e85-bd16-aaa29632f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1. What is the purpose of forward propagation in a neural network?\n",
    "\n",
    "    Ans: The purpose of forward propagation in a neural network is to process input data through the network's layers, applying weights, biases, \n",
    "         and activation functions, to generate predictions or outputs. It calculates the information flow from the input to the output, enabling \n",
    "         the network to make predictions for given inputs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3165260-4fd9-4a4d-b22f-d506a151b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?\n",
    "\n",
    "    Ans: Steps are as follows: \n",
    "            1 .Initialize the input features as a vector, denoted as X.\n",
    "            2 .Initialize the weight matrix W and bias vector b for the single layer.\n",
    "            3 .Calculate the weighted sum of inputs for each neuron in the layer:\n",
    "                Z = X * W^T + b\n",
    "            4. Apply the activation function (e.g., ReLU, sigmoid, tanh) element-wise to the result of the weighted sum:\n",
    "                A = activation_function(Z)\n",
    "            5. The output of the single-layer network will be the vector A, representing the predictions or activations for each neuron in the layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cfdbe7-c2e5-444e-9cb5-4e908dd68f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q3. How are activation functions used during forward propagation?\n",
    "\n",
    "    Ans: During forward propagation, activation functions are applied to the weighted sum of inputs in each neuron. \n",
    "         They introduce non-linearity to the neural network, enabling it to model complex relationships in data and produce \n",
    "         meaningful predictions or activations for the subsequent layers in the network.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0676f-a2be-4920-b375-d8cf7fc48e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q4. What is the role of weights and biases in forward propagation?\n",
    "\n",
    "    Ans: In forward propagation, weights determine the strength of connections between neurons, influencing the impact of inputs on each neuron's output. \n",
    "         Biases act as additional input to neurons, adjusting their responsiveness. \n",
    "         Together, they control how information flows through the neural network, leading to accurate predictions or activations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2e84c-95cd-4c61-beb4-0e14baa6acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?\n",
    "\n",
    "    Ans: The purpose of applying a softmax function in the output layer during forward propagation is to convert raw scores into probability distributions. \n",
    "         It ensures the output values represent the likelihood of each class, making it easier to interpret and choose the most probable class for multi-class \n",
    "         classification tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7022e31-9a29-4480-b3da-bf15be7779c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q6. What is the purpose of backward propagation in a neural network?\n",
    "\n",
    "    Ans: The purpose of backward propagation in a neural network is to adjust the model's weights and biases based on the error between predicted and actual outputs. \n",
    "         It calculates gradients to update parameters, enabling the network to learn from mistakes and improve its performance during training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389eb30b-cbee-49e5-ad9a-454b7419c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q7. How is backward propagation mathematically calculated in a single-layer feedforward neural network?\n",
    "\n",
    "    Ans: In a single-layer feedforward neural network, backward propagation calculates gradients of the loss function with respect to the weights \n",
    "         and biases of the layer. It involves using the chain rule to propagate the error from the output layer to the input layer, enabling weight \n",
    "         and bias updates to improve the model's predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953cfe3b-d228-4ff6-83b8-84f0e51e9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q8. Can you explain the concept of the chain rule and its application in backward propagation?\n",
    "\n",
    "    Ans: The chain rule is a calculus principle that allows us to find the derivative of a composite function. \n",
    "         In backward propagation, it's used to efficiently calculate gradients by propagating the error from the output layer back through the network, \n",
    "         updating weights and biases to improve the model's performance during training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a31b11-4c74-4017-b129-de26f5343b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q9. What are some common challenges or issues that can occur during backward propagation, and how can they be addressed?\n",
    "\n",
    "    Ans: Some common challenges in backward propagation are vanishing or exploding gradients, which can slow down or destabilize training. \n",
    "         They can be addressed using techniques like weight initialization, gradient clipping, or using activation functions like ReLU to \n",
    "         mitigate these problems and ensure stable and efficient learning.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
